<img width="1768" height="716" alt="image" src="https://github.com/user-attachments/assets/a7241bea-dd53-466e-967f-1a97fbb3206e" />## üöÄ Como Executar as Quest√µes no Google Colab

O projeto est√° dividido em m√∫ltiplos notebooks, cada um correspondendo a uma quest√£o ou etapa da an√°lise. Todos foram preparados para rodar diretamente no Google Colab.

Abaixo voc√™ encontrar√° o mapeamento de cada notebook, os dados que ele utiliza e um link direto para abri-lo no Colab.

### Mapeamento de Quest√µes e Datasets

| Arquivo do Notebook | Dataset(s) Utilizado(s)             | Descri√ß√£o da Quest√£o                                       | Link Direto para o Colab                                                                                                                                                             |
| ------------------- | ----------------------------------- | ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Q5.ipynb** | `heart_desease_data.csv`     | <img width="1215" height="569" alt="image" src="https://github.com/user-attachments/assets/8becbb15-4e47-46b5-94e7-4c3cbb54714c" /> | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Eduardo-Mati/Projetos-integrador---Desafio-unifacisa---Modelos-de-Machine-Learning/blob/main/Q5.ipynb) |
| **Q6.ipynb** | `California Housing do Scikit-Learn, j√° presente importado no c√≥digo`     | <img width="1199" height="493" alt="image" src="https://github.com/user-attachments/assets/937f351c-7431-4eeb-a88c-257fcd63565e" /> | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Eduardo-Mati/Projetos-integrador---Desafio-unifacisa---Modelos-de-Machine-Learning/blob/main/Q6.ipynb) |
| **Q7.ipynb** | `Groceries_dataset.csv`     | <img width="1161" height="465" alt="image" src="https://github.com/user-attachments/assets/aef20693-4f65-4ac9-a84f-50c8b47fb633" /> | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Eduardo-Mati/Projetos-integrador---Desafio-unifacisa---Modelos-de-Machine-Learning/blob/main/Q7.ipynb) |
| **Q8.ipynb** | `avaliacoes_filmes.csv`     | <img width="1178" height="517" alt="image" src="https://github.com/user-attachments/assets/eb281138-b7ef-4da9-b1fb-cc78a44664ef" /> | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Eduardo-Mati/Projetos-integrador---Desafio-unifacisa---Modelos-de-Machine-Learning/blob/main/Q8.ipynb) |
| **Q9.ipynb** | `paultimothymooney/chest-xray-pneumonia j√° importado dentro do c√≥digo`     | <img width="1191" height="535" alt="image" src="https://github.com/user-attachments/assets/b80529ba-60eb-43c4-938a-4a1227973309" /> | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Eduardo-Mati/Projetos-integrador---Desafio-unifacisa---Modelos-de-Machine-Learning/blob/main/Q9.ipynb) |
| **Q10.ipynb** | `store.csv`  | <img width="918" height="732" alt="image" src="https://github.com/user-attachments/assets/4fcbfef3-8f04-4310-81e0-13e36c55c976" /> | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Eduardo-Mati/Projetos-integrador---Desafio-unifacisa---Modelos-de-Machine-Learning/blob/main/Q10.ipynb) |


### Instru√ß√µes de Execu√ß√£o

1.  **Escolha a Quest√£o:** Na tabela acima, clique no bot√£o "Open In Colab" correspondente √† quest√£o que voc√™ deseja executar.
2.  **Execute a C√©lula de Configura√ß√£o:** Como antes, a **primeira c√©lula** de cada notebook deve ser executada primeiro para clonar o reposit√≥rio e instalar as bibliotecas. Isso garante que o notebook ter√° acesso aos arquivos de dataset corretos.
3.  **Execute o Restante do C√≥digo:** Ap√≥s a configura√ß√£o, execute as outras c√©lulas em sequ√™ncia para ver a an√°lise completa daquela quest√£o.

### Funcionamento do c√≥digo por etapa


Quest√£o 5 (Intermedi√°rio) - Diagn√≥stico de Doen√ßas Card√≠aca


<img width="585" height="851" alt="image" src="https://github.com/user-attachments/assets/0a36d36a-3398-4010-bd09-8d8f25f8310c" />

 
1. Importa√ß√£o das Bibliotecas
O c√≥digo √© organizado em se√ß√µes para importar diferentes tipos de ferramentas:

‚Ä¢	Manipula√ß√£o de Dados:

 o	import pandas as pd: A biblioteca principal para carregar e manipular os dados em tabelas (DataFrames).

 o	import numpy as np: Usada para opera√ß√µes matem√°ticas e num√©ricas.

‚Ä¢	Visualiza√ß√£o:

 o	import matplotlib.pyplot as plt e import seaborn as sns: Duas bibliotecas usadas para criar gr√°ficos e visualizar os dados.

‚Ä¢	Pr√©-processamento:

 o	from sklearn.model_selection import train_test_split: Fun√ß√£o essencial para dividir os dados em conjuntos de treino e teste.

 o	from sklearn.preprocessing import ...: Importa ferramentas para "limpar" os dados antes de us√°-los:

  ÔÇß	StandardScaler, MinMaxScaler: Para normalizar ou padronizar as colunas num√©ricas (coloc√°-las na mesma escala).

  ÔÇß	OneHotEncoder: Para converter colunas de texto (categ√≥ricas) em n√∫meros que o modelo possa entender.

 o	from sklearn.impute import SimpleImputer: Ferramenta para preencher valores ausentes (NaN) nos dados.

‚Ä¢	Modelos de Classifica√ß√£o:

 o	from sklearn.ensemble import RandomForestClassifier: Importa o modelo Random Forest, um modelo robusto baseado em √°rvores de decis√£o.

 o	from sklearn.svm import SVC: Importa o Support Vector Machine (SVM), outro modelo de classifica√ß√£o poderoso.

 o	from sklearn.linear_model import LogisticRegression: Importa a Regress√£o Log√≠stica, um terceiro modelo (linear) que tamb√©m √© √≥timo para classifica√ß√£o.

‚Ä¢	M√©tricas de Avalia√ß√£o:

 o	from sklearn.metrics import ...: Importa todas as fun√ß√µes necess√°rias para avaliar o desempenho dos modelos, conforme solicitado na atividade: accuracy_score (acur√°cia), precision_score (precis√£o), recall_score (recall), f1_score, roc_auc_score (curva ROC-AUC) e confusion_matrix (matriz de confus√£o).

‚Ä¢	Explicabilidade (XAI):

 o	import shap: Importa a biblioteca SHAP. Esta √© uma ferramenta avan√ßada que ser√° usada para responder √† segunda pergunta da atividade: "Quais vari√°veis mais impactaram na previs√£o?".

‚Ä¢	Configura√ß√µes Adicionais:

 o	sns.set_style(...): Define um estilo visual (whitegrid) para todos os gr√°ficos do Seaborn.

3. Carregamento dos Dados

‚Ä¢	df = pd.read_csv('heart_desease_data.csv'): Este comando usa o Pandas para ler o arquivo heart_desease_data.csv e o armazena na vari√°vel df (DataFrame), que √© a nossa tabela principal de trabalho.

<img width="886" height="493" alt="image" src="https://github.com/user-attachments/assets/af3b4d66-8179-4aa2-9654-1faa8a85508c" />

 
Este bloco de c√≥digo est√° preparando os dados para o modelo de machine learning.

1.	Ele primeiro separa o dataset em duas partes: as features (dados de entrada, como idade, colesterol) e o target (o que queremos prever: se tem ou n√£o doen√ßa).

2.	Em seguida, ele usa o StandardScaler para padronizar todas as features.

3.	Por que fazer isso? A padroniza√ß√£o coloca todas as vari√°veis na mesma escala (m√©dia 0, desvio padr√£o 1). Isso √© crucial para que modelos como o SVM funcionem corretamente e n√£o deem import√¢ncia indevida a vari√°veis que possuem n√∫meros grandes (como 'colesterol') em vez de vari√°veis com n√∫meros pequenos (como 'sex').

 <img width="886" height="388" alt="image" src="https://github.com/user-attachments/assets/03ef4bbd-5ad4-4a59-9831-02c4a7a874ea" />

Este bloco de c√≥digo define uma fun√ß√£o chamada plot_corr para visualizar a matriz de correla√ß√£o dos dados.

1.	corr = df_scaled.corr(): Este comando calcula a matriz de correla√ß√£o. Ela mostra um valor (entre -1 e 1) que mede a for√ßa da rela√ß√£o linear entre cada par de vari√°veis (por exemplo, como a "idade" se correlaciona com o "colesterol").

2.	ax.matshow(corr): Este comando usa o Matplotlib para desenhar um mapa de calor (heatmap) dessa matriz. As cores no gr√°fico representam os valores da correla√ß√£o (cores quentes para correla√ß√£o positiva, frias para negativa).

3.	plot_corr(df_scaled): Finalmente, o c√≥digo chama a fun√ß√£o para exibir esse mapa de calor.

Por que fazer isso? Para entender visualmente quais features (vari√°veis) est√£o fortemente conectadas umas com as outras, o que pode ser importante para a sele√ß√£o de features e para entender o comportamento do dataset.

<img width="886" height="528" alt="image" src="https://github.com/user-attachments/assets/b71832b9-0e13-4b69-9dbb-02c1e7841f40" />

Este bloco de c√≥digo realiza duas verifica√ß√µes cruciais da qualidade e integridade dos dados ap√≥s a padroniza√ß√£o:

1.	df_scaled.isnull().sum()

 o	O que faz: Este comando verifica cada coluna do DataFrame df_scaled e conta quantos valores nulos (ausentes, NaN) existem em cada uma.

 o	Por que fazer isso: Modelos de machine learning n√£o conseguem lidar com dados ausentes. A sa√≠da (que mostra 0 para todas as colunas) √© uma excelente not√≠cia, pois confirma que n√£o h√° valores nulos e que n√£o precisaremos de t√©cnicas de imputa√ß√£o (preenchimento de dados faltantes).

2.	df_scaled.describe()

 o	O que faz: Este comando gera um resumo estat√≠stico detalhado para cada coluna num√©rica.

 o	Por que fazer isso: Esta √© a prova final de que o StandardScaler (do bloco anterior) funcionou perfeitamente. Podemos observar na sa√≠da:

  ÔÇß	mean (m√©dia): Todos os valores da m√©dia s√£o n√∫meros extremamente pequenos, muito pr√≥ximos de 0 (ex: 4.690515e-17, que √© 0.00...0469).

  ÔÇß	std (desvio padr√£o): Todos os valores do desvio padr√£o s√£o exatamente 1.001654e+00 (que √© 1.001654, um valor muito pr√≥ximo de 1, a pequena diferen√ßa se deve ao c√°lculo amostral do pandas).

 o	Isso confirma que todas as features est√£o agora na mesma escala padronizada, prontas para serem usadas pelos modelos.

<img width="886" height="566" alt="image" src="https://github.com/user-attachments/assets/94432f73-d26b-4710-ad29-28fa7c386a92" />

 
Este bloco de c√≥digo realiza duas tarefas principais: dividir os dados em conjuntos de treino/teste e treinar o primeiro modelo de classifica√ß√£o.

1.	Prepara√ß√£o dos Dados para sklearn:

 o	data_treino = np.array(df_scaled): Converte o DataFrame df_scaled (as features padronizadas) em um array NumPy.

 o	data_classif = np.array(target): Converte a s√©rie target (as respostas "0" ou "1") em um array NumPy.

 o	Por qu√™? As bibliotecas do sklearn (como train_test_split e os modelos) s√£o otimizadas para trabalhar com arrays NumPy.

2.	Defini√ß√£o da Base de Treino e Teste:

 o	x_treino, x_val, y_treino, y_val = train_test_split(...): Este comando divide os dados em quatro conjuntos:

  ÔÇß	x_treino e y_treino: 70% dos dados, que ser√£o usados para treinar o modelo.

  ÔÇß	x_val e y_val: 30% dos dados (definido por test_size=0.30), que ser√£o guardados para testar o modelo.

 o	Por qu√™? Esta √© a etapa mais importante para uma avalia√ß√£o honesta. O modelo s√≥ pode "aprender" com os dados de treino. Os dados de teste (x_val, y_val) s√£o usados no final para ver o qu√£o bem o modelo generaliza para dados que ele nunca viu.

3.	Imprimindo os Resultados (Verifica√ß√£o):

 o	Os comandos print(...) apenas calculam e mostram as porcentagens da divis√£o (69.97% treino, 30.03% teste), confirmando que a separa√ß√£o funcionou.

4.	Treinando o Modelo Random Forest:

 o	rf = RandomForestClassifier(random_state=42): Inicializa o primeiro modelo, o RandomForestClassifier. O random_state=42 √© usado para garantir que os resultados sejam reproduz√≠veis.

 o	rf.fit(x_treino, y_treino): Este √© o comando de treinamento. O modelo rf "aprende" os padr√µes nos dados de treino (x_treino) para prever as respostas (y_treino).

<img width="886" height="908" alt="image" src="https://github.com/user-attachments/assets/85af8f11-ab03-4985-841a-339dbab26e89" />

 
Este bloco de c√≥digo avalia o desempenho do modelo RandomForestClassifier que acabamos de treinar.

Ele faz duas avalia√ß√µes distintas:

1.	Avalia√ß√£o nos Dados de TREINO:

 o	y_pred_treino = rf.predict(x_treino): O modelo faz previs√µes sobre os pr√≥prios dados que usou para treinar.

 o	accuracy_score(y_treino, y_pred_treino): Compara as previs√µes com as respostas corretas.

 o	Resultado (1.0): A acur√°cia foi de 100%. Isso indica que o modelo "decorou" perfeitamente os dados de treino, um sinal cl√°ssico de overfitting. O que importa de verdade √© o teste abaixo.

2.	Avalia√ß√£o nos Dados de TESTE (Valida√ß√£o):

 o	y_pred_val = rf.predict(x_val): O modelo faz previs√µes sobre os dados de teste (x_val), que ele nunca viu antes. Este √© o teste real.

 o	confusion_matrix(...): Gera a Matriz de Confus√£o.

  ÔÇß	A sa√≠da [[41 9] [11 30]] significa (para as classes [1, 0]):

  ÔÇß	41 Verdadeiros Positivos: Acertou 41 pacientes que tinham doen√ßa.

  ÔÇß	9 Falsos Negativos: Errou 9 pacientes que tinham doen√ßa (disse que n√£o tinham).

  ÔÇß	11 Falsos Positivos: Errou 11 pacientes que n√£o tinham doen√ßa (disse que tinham).

  ÔÇß	30 Verdadeiros Negativos: Acertou 30 pacientes que n√£o tinham doen√ßa.

 o	classification_report(...): Gera o relat√≥rio completo de m√©tricas.

  ÔÇß	accuracy (Acur√°cia Total): 0.78 (O modelo acertou 78% de todas as previs√µes).

  ÔÇß	precision (classe 1): 0.79 (Quando o modelo disse "tem doen√ßa", ele estava certo 79% das vezes).

  ÔÇß	recall (classe 1): 0.82 (O modelo conseguiu identificar 82% de todos os pacientes que realmente tinham a doen√ßa).

<img width="600" height="827" alt="image" src="https://github.com/user-attachments/assets/69ec9de8-acd4-40e9-b1b4-6e4115d3f734" />

 
Este bloco de c√≥digo treina e avalia o segundo modelo da atividade, o Support Vector Machine (SVM), para que possamos compar√°-lo com o Random Forest.

1.	Treinando o Modelo SVM:

 o	svm = SVC(random_state=42): Inicializa o modelo SVC (Support Vector Classifier).

 o	svm.fit(x_treino, y_treino): Treina o modelo SVM usando os mesmos dados de treino (x_treino, y_treino) que o Random Forest usou.

2.	Avalia√ß√£o nos Dados de TREINO:

 o	y_pred_treino = svm.predict(x_treino): O modelo faz previs√µes sobre os dados de treino.

 o	accuracy_score(...): Calcula a acur√°cia.

 o	Resultado (0.929...): A acur√°cia foi de 93%. Isso √© alto, mas (ao contr√°rio do Random Forest) n√£o √© 100%, o que sugere que o SVM est√° "decorando" menos os dados de treino e pode ter uma capacidade de generaliza√ß√£o melhor.

3.	Avalia√ß√£o nos Dados de TESTE (Valida√ß√£o):

 o	y_pred_val = svm.predict(x_val): O modelo SVM faz previs√µes sobre os dados de teste (x_val).

 o	confusion_matrix(...): Gera a Matriz de Confus√£o.

  ÔÇß	A sa√≠da [[43 7] [14 27]] significa (para as classes [1, 0]):

  ÔÇß	43 Verdadeiros Positivos: Acertou 43 pacientes que tinham doen√ßa.

  ÔÇß	7 Falsos Negativos: Errou 7 pacientes que tinham doen√ßa (disse que n√£o tinham).

  ÔÇß	14 Falsos Positivos: Errou 14 pacientes que n√£o tinham doen√ßa (disse que tinham).

  ÔÇß	27 Verdadeiros Negativos: Acertou 27 pacientes que n√£o tinham doen√ßa.

 o	classification_report(...): Gera o relat√≥rio de m√©tricas.

  ÔÇß	accuracy (Acur√°cia Total): 0.77 (O modelo acertou 77% de todas as previs√µes).

  ÔÇß	precision (classe 1): 0.75 (Quando o modelo disse "tem doen√ßa", ele estava certo 75% das vezes).

  ÔÇß	recall (classe 1): 0.86 (O modelo conseguiu identificar 86% de todos os pacientes que realmente tinham a doen√ßa).

Compara√ß√£o r√°pida (SVM vs. RF): O SVM teve uma acur√°cia total ligeiramente menor (77% vs 78% do RF), mas alcan√ßou um Recall maior (86% vs 82%), o que significa que foi um pouco melhor em encontrar os pacientes que de fato estavam doentes.

<img width="886" height="498" alt="image" src="https://github.com/user-attachments/assets/1082899f-67ae-4790-a154-abeb000abba7" />

 
Este bloco de c√≥digo responde √† segunda pergunta da atividade: "Quais vari√°veis mais impactaram na previs√£o?", especificamente para o modelo Random Forest.

1.	importances = rf.feature_importances_:

 o	O que faz: O modelo Random Forest (rf), ap√≥s ser treinado, armazena um "score" de import√¢ncia para cada feature (coluna) em um atributo chamado .feature_importances_. Esse score mede o quanto cada feature (idade, sexo, colesterol, etc.) contribuiu para tomar as decis√µes corretas.

2.	indices = np.argsort(importances)[::-1]:

 o	O que faz: Este comando pega a lista de scores (importances) e a ordena, mas em vez de retornar os scores ordenados, ele retorna os √≠ndices (posi√ß√µes) das colunas, da mais importante para a menos importante.

  ÔÇß	np.argsort() ordena do menor para o maior.

  ÔÇß	[::-1] inverte a lista, para que o mais importante venha primeiro.

3.	print(indices):

 o	O que faz: Imprime o ranking das features por import√¢ncia.

 o	Sa√≠da: [ 2 7 9 0 11 12 4 3 10 8 1 6 5]

 o	O que significa? Usando a tabela df_scaled como refer√™ncia (onde 'age' √© o √≠ndice 0, 'sex' √© o 1, 'cp' √© o 2, etc.), este ranking nos diz que:

  ÔÇß	A feature mais importante √© a de √≠ndice 2 (cp - tipo de dor no peito).

  ÔÇß	A segunda mais importante √© a de √≠ndice 7 (thalach - frequ√™ncia card√≠aca m√°xima).

  ÔÇß	A terceira mais importante √© a de √≠ndice 9 (oldpeak).

  ÔÇß	A quarta mais importante √© a de √≠ndice 0 (age).

 e assim por diante.

<img width="886" height="652" alt="image" src="https://github.com/user-attachments/assets/056fac18-556d-425f-9119-9b7bce70f39e" />

 
Este bloco de c√≥digo instala uma nova biblioteca (eli5) para responder √† mesma pergunta ("Quais vari√°veis mais impactaram?"), mas desta vez para o modelo SVM.

O SVM n√£o possui um atributo .feature_importances_ simples como o Random Forest, por isso √© necess√°rio usar uma t√©cnica diferente chamada "Import√¢ncia por Permuta√ß√£o".

1.	%pip install eli5: Instala a biblioteca eli5, que √© especializada em "explicar" modelos de machine learning.

2.	from eli5.sklearn import PermutationImportance: Importa a ferramenta de "Import√¢ncia por Permuta√ß√£o".

3.	perm = PermutationImportance(svm, ...).fit(x_val, y_val): Este √© o comando principal.

 o	O que faz: Ele usa o modelo svm treinado e os dados de teste (x_val, y_val).

 o	Como funciona:

1.	Primeiro, ele mede a acur√°cia do SVM nos dados de teste (a "pontua√ß√£o base").

2.	Depois, ele pega uma coluna (ex: 'age'), embaralha os valores dela aleatoriamente e mede a acur√°cia de novo.

3.	Se a acur√°cia cair muito, significa que essa coluna era muito importante.

4.	Se a acur√°cia n√£o mudar (ou at√© melhorar um pouco), significa que a coluna era irrelevante ou prejudicial.

 o	Ele repete esse processo para todas as colunas.

4.	eli5.show_weights(perm, feature_names=feature_names): Este comando exibe os resultados em uma tabela formatada.

O Resultado (A Tabela):

A tabela mostra o "Peso" (o quanto a acur√°cia do modelo caiu quando a feature foi embaralhada).

‚Ä¢	Para o SVM, a feature mais importante foi ca (n√∫mero de vasos principais).

‚Ä¢	A segunda foi thal.

‚Ä¢	A terceira foi cp (tipo de dor no peito).

‚Ä¢	Interessante notar que chol (colesterol) teve uma import√¢ncia negativa (-0.0223), o que sugere que essa feature pode ter mais confundido o SVM do que ajudado.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Quest√£o 6 (Avan√ßado) ‚Äì Previs√£o do Valor de Im√≥veis

Trabalho desenvolvido no Google Colab utilizando t√©cnicas de Machine Learning para prever o valor m√©dio de im√≥veis na Calif√≥rnia, com base no dataset California Housing.

Bibliotecas usadas

Antes de executar o c√≥digo, foram utilizadas as seguintes bibliotecas (algumas precisam ser instaladas):

- pandas: manipula√ß√£o e an√°lise de dados (DataFrame).
- numpy: opera√ß√µes num√©ricas e arrays.
- matplotlib.pyplot: cria√ß√£o de gr√°ficos.
- seaborn: aprimora visualiza√ß√µes estat√≠sticas (estilos e gr√°ficos).
- scikit-learn (sklearn): carregamento do dataset e ferramentas de pr√©-processamento, divis√£o de dados e modelos (fetch_california_housing, train_test_split, StandardScaler, LinearRegression, m√©tricas como mean_squared_error e r2_score).
- xgboost: algoritmo de boosting baseado em √°rvores (XGBRegressor).
- tensorflow / keras: constru√ß√£o e treinamento de redes neurais (Sequential, Dense).

Comandos usados para instalar (exemplo):
`!pip install pandas numpy scikit-learn matplotlib seaborn xgboost tensorflow`

Essas bibliotecas permitem carregar, preparar, visualizar os dados e aplicar diferentes modelos de regress√£o.

<img width="900" height="468" alt="image" src="https://github.com/user-attachments/assets/a72a9225-1889-4519-8754-fe99a2cd3c46" />

 
Aqui iniciamos o projeto importando o dataset California Housing, que cont√©m informa√ß√µes sobre a renda, localiza√ß√£o e caracter√≠sticas das casas da Calif√≥rnia.

<img width="900" height="591" alt="image" src="https://github.com/user-attachments/assets/9ae7ce7e-6f83-4ec6-8e5e-9a1a6e355aca" />

 
Nesta parte, s√£o instaladas e importadas as bibliotecas necess√°rias, como Pandas, NumPy, Seaborn, Matplotlib, Scikit-Learn, XGBoost e TensorFlow.

<img width="900" height="463" alt="image" src="https://github.com/user-attachments/assets/a1bf6511-3dc1-4f5d-8066-30baefeecb23" />

 
Aqui configuramos o estilo visual dos gr√°ficos com o Seaborn e o Matplotlib, deixando os resultados mais claros e bonitos.

<img width="900" height="417" alt="image" src="https://github.com/user-attachments/assets/056fc582-1fee-4b41-9433-b9dc0a7f6783" />

 
Nesta c√©lula, ocorre o pr√©-processamento dos dados: normaliza√ß√£o e separa√ß√£o entre treino e teste para preparar o dataset.


<img width="900" height="422" alt="image" src="https://github.com/user-attachments/assets/49163ab5-43b7-4651-a650-c5dcbb7068d2" />

 
Aqui visualizamos as primeiras linhas do dataset e analisamos o significado de cada coluna, como renda mediana, idade m√©dia das casas e n√∫mero m√©dio de c√¥modos.


<img width="900" height="186" alt="image" src="https://github.com/user-attachments/assets/ca10c488-47dc-49c8-b353-3e10f44a0485" />

 
Nesta parte, o c√≥digo faz a normaliza√ß√£o com o StandardScaler, deixando os dados padronizados para os modelos.


<img width="900" height="349" alt="image" src="https://github.com/user-attachments/assets/c7fdbcb7-8430-4f28-8bfb-e929679468aa" />

 
A seguir, √© criada uma matriz de correla√ß√£o, que mostra quais vari√°veis t√™m mais influ√™ncia sobre o valor das casas.


<img width="900" height="205" alt="image" src="https://github.com/user-attachments/assets/06c2102c-ed30-48b8-bba6-1bdf9e5372ef" />

 
Aqui os dados s√£o divididos entre treino (80%) e teste (20%) para avaliar o desempenho dos modelos.


<img width="900" height="384" alt="image" src="https://github.com/user-attachments/assets/a22eb35d-a126-45ef-8aed-95abf36d7c62" />

 
O primeiro modelo utilizado √© a Regress√£o Linear, que apresentou um R¬≤ de aproximadamente 0.57, explicando parte da varia√ß√£o dos pre√ßos.


<img width="900" height="342" alt="image" src="https://github.com/user-attachments/assets/6c9d3033-240b-402a-a688-1301aff48f16" />

 
Depois testamos o XGBoost e uma Rede Neural Artificial, obtendo melhor desempenho ‚Äî especialmente o XGBoost, com um R¬≤ de 0.83.


<img width="900" height="267" alt="image" src="https://github.com/user-attachments/assets/7394a460-0da0-4460-995c-18ed7c321c04" />

 
Aqui est√° a avalia√ß√£o final da Rede Neural ap√≥s treinamento: o RMSE final foi de aproximadamente 0.439 e o R¬≤ atingiu cerca de 0.80, mostrando bom desempenho da rede.


<img width="900" height="266" alt="image" src="https://github.com/user-attachments/assets/a786ac46-9d6a-466e-a5a2-22cd1034ffb6" />

 
Pergunta/Resposta: Qual modelo teve menor erro de previs√£o? Como otimizar ainda mais o desempenho?

- O modelo com menor erro de previs√£o foi o XGBoost, com RMSE em torno de 0.40 e R¬≤ de aproximadamente 0.83.

- Para melhorar ainda mais o desempenho, algumas a√ß√µes poss√≠veis s√£o:

  ‚Ä¢ Fazer ajuste de hiperpar√¢metros (grid search ou randomized search) em modelos como XGBoost e na Rede Neural.

  ‚Ä¢ Realizar engenharia de features: criar novas vari√°veis relevantes, tratar outliers e explorar intera√ß√µes entre vari√°veis.

  ‚Ä¢ Coletar ou incluir mais dados/contexto (por exemplo, informa√ß√µes sobre zoneamento, proximidade a servi√ßos, etc.).

  ‚Ä¢ Usar valida√ß√£o cruzada (cross-validation) para ter estimativas mais robustas do desempenho.

  ‚Ä¢ Experimentar ensembles de modelos e t√©cnicas de regulariza√ß√£o para reduzir overfitting.


-------------------------------------------------------------------------------------------------------------------------------------

Quest√£o 7 (Intermedi√°rio) - Recomenda√ß√£o de Produtos em um Supermercado

Bibliotecas Utilizadas
- pandas
- mlxtend.frequent_patterns
- numpy
- matplotlib.pyplot
- seaborn
- warnings


<img width="900" height="332" alt="image" src="https://github.com/user-attachments/assets/fa58f6c2-0c43-40c4-9190-3f1a0dd95df6" />

Neste primeiro trecho, importamos as bibliotecas necess√°rias para an√°lise de dados, visualiza√ß√£o e cria√ß√£o das regras de associa√ß√£o. O 'pandas' gerencia os dados em formato de tabela; 'mlxtend' traz o algoritmo Apriori; 'numpy' oferece suporte matem√°tico; 'matplotlib' e 'seaborn' ajudam na visualiza√ß√£o; e 'warnings' √© usado para suprimir avisos desnecess√°rios.


<img width="900" height="306" alt="image" src="https://github.com/user-attachments/assets/7c99fd0d-2a01-4e92-a4a6-55f5c20e72bb" />


Aqui, o c√≥digo l√™ o arquivo CSV com os dados das compras no supermercado, criando um DataFrame. A fun√ß√£o df.head(10) mostra as primeiras linhas para termos uma vis√£o inicial dos dados ‚Äî contendo o n√∫mero do cliente, a data e o item comprado.

<img width="900" height="348" alt="image" src="https://github.com/user-attachments/assets/7c299ad8-e3ac-425f-ba06-a7dfd9ee2783" />

 
Nesta etapa, removemos a coluna 'Date', pois a data da compra n√£o √© relevante para descobrir rela√ß√µes entre produtos. A fun√ß√£o df.drop elimina essa coluna do DataFrame.

<img width="900" height="295" alt="image" src="https://github.com/user-attachments/assets/7e03adf6-3536-4ce5-bcdc-159df5526dcd" />

 
Agora, renomeamos as colunas para facilitar o entendimento: 'Member_number' vira 'ID' e 'itemDescription' vira 'Itens'. Isso deixa o dataset mais limpo e intuitivo para an√°lise.

<img width="900" height="313" alt="image" src="https://github.com/user-attachments/assets/35579733-20f0-4c79-8430-ab99fa1ee4ac" />

 
Em seguida, agrupamos os dados por cliente (ID) e unimos todos os itens comprados em uma √∫nica linha, separados por v√≠rgula. Assim, temos uma vis√£o do conjunto de produtos adquiridos por cada pessoa, o que √© essencial para o algoritmo Apriori identificar padr√µes de compra.

<img width="900" height="316" alt="image" src="https://github.com/user-attachments/assets/93a25229-96b6-45f1-b6e4-48b89a98abee" />

 
Aqui aplicamos o algoritmo Apriori com suporte m√≠nimo de 2%. Isso significa que s√≥ consideramos combina√ß√µes de produtos que aparecem em pelo menos 2% das transa√ß√µes. Em seguida, geramos as regras de associa√ß√£o com base no 'lift' e ordenamos as regras mais fortes primeiro.

<img width="900" height="245" alt="image" src="https://github.com/user-attachments/assets/d52c99b6-8872-4c4f-81d4-fe22d366ef36" />

 
Por fim, temos a an√°lise dos resultados. A tabela mostra as combina√ß√µes de produtos (antecedentes e consequentes) e as m√©tricas que indicam a for√ßa da rela√ß√£o entre eles, como suporte, confian√ßa e lift. Isso nos permite entender quais produtos tendem a ser comprados juntos.

<img width="900" height="200" alt="image" src="https://github.com/user-attachments/assets/203b2661-8f6e-435e-a99f-1d6f340fdf8b" />

 
Na conclus√£o textual, respondemos √† pergunta sobre as regras mais relevantes e como aplic√°-las no mercado. A ideia √© que os produtos mais frequentemente comprados juntos sejam colocados pr√≥ximos nas prateleiras, estimulando vendas cruzadas e aumentando o faturamento.

Conclus√£o
Neste projeto, aprendemos a aplicar o algoritmo Apriori para identificar padr√µes de compra em um supermercado. Com isso, √© poss√≠vel recomendar produtos e planejar estrat√©gias de vendas baseadas em dados. O uso de regras de associa√ß√£o mostrou como a an√°lise de dados pode gerar insights pr√°ticos para o com√©rcio.

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Quest√£o 8 (Avan√ßado) - Recomenda√ß√£o de Filmes com Filtragem Colaborativa


<img width="886" height="360" alt="image" src="https://github.com/user-attachments/assets/fc9890ae-33c7-49a7-9f76-a6ae88406ae0" />

 
Este bloco de c√≥digo √© a Etapa 0: Configura√ß√£o do Ambiente e Instala√ß√£o de Bibliotecas.

Aqui, o c√≥digo usa o comando !pip install para garantir que todas as ferramentas necess√°rias para o projeto estejam instaladas no ambiente.

‚Ä¢	!pip install mlxtend: Instala a biblioteca mlxtend, que √© muito usada para Regras de Associa√ß√£o (como o algoritmo Apriori, que descobre quais itens s√£o frequentemente comprados juntos).

‚Ä¢	!pip install tensorflow, !pip install keras, !pip install tensorflow-recommenders: Instalam o TensorFlow e o Keras, que s√£o as principais bibliotecas para Deep Learning, e o tensorflow-recommenders, um pacote espec√≠fico para construir sistemas de recomenda√ß√£o avan√ßados.

‚Ä¢	import pandas as pd, import numpy as np: Importa as duas bibliotecas mais fundamentais para manipula√ß√£o e an√°lise de dados no Python.


<img width="532" height="492" alt="image" src="https://github.com/user-attachments/assets/8cc8a683-9462-477a-bda5-20212a8e5a9c" />

 
Neste bloco, o c√≥digo est√° carregando o dataset principal para o sistema de recomenda√ß√£o.

‚Ä¢	O que faz: O comando pd.read_csv('avaliacoes_filmes.csv') l√™ o arquivo CSV que cont√©m o hist√≥rico de avalia√ß√µes dos usu√°rios e o armazena em uma estrutura de tabela chamada DataFrame, que √© nomeada df.

‚Ä¢	Por que: Esta √© a etapa inicial e essencial. Precisamos carregar os dados na mem√≥ria para poder analis√°-los e us√°-los para treinar o modelo.

‚Ä¢	A Sa√≠da: df.head(10) √© usado para "espiar" as 10 primeiras linhas do arquivo. Isso confirma que os dados foram carregados corretamente e nos permite ver a estrutura: temos um user_id (o usu√°rio), um movie_id (o filme), um rating (a nota que o usu√°rio deu) e um timestamp (quando a avalia√ß√£o foi feita).


<img width="759" height="1459" alt="image" src="https://github.com/user-attachments/assets/13714db2-02bf-475f-b8cc-755a94034c0f" />


 
Este bloco de c√≥digo √© o n√∫cleo da Filtragem Colaborativa Cl√°ssica. Ele constr√≥i, treina e avalia dois modelos diferentes para comparar suas performances:

1.	Filtragem Colaborativa Baseada no Usu√°rio (User-Based)

2.	Filtragem Colaborativa Baseada no Item (Item-Based)

Tudo isso √© feito usando a biblioteca surprise, que √© especializada nesse tipo de sistema de recomenda√ß√£o.
________________________________________

An√°lise Detalhada do Bloco:

1. Prepara√ß√£o dos Dados para a Biblioteca surprise

‚Ä¢	O que faz: Antes de usar o surprise, o DataFrame do Pandas precisa ser convertido.

o	Reader(rating_scale=(1, 5)): Primeiro, √© criado um Reader para informar √† biblioteca que as notas (ratings) no dataset variam de 1 a 5.

o	Dataset.load_from_df(...): Carrega os dados do DataFrame (df), especificando quais colunas s√£o o user_id, movie_id e rating.

o	train_test_split(...): Divide o conjunto de dados em 75% para treino e 25% para teste (test_size=.25).

‚Ä¢	Por que: O surprise tem seu pr√≥prio formato de dados otimizado. A divis√£o treino/teste √© fundamental para avaliar se o modelo √© bom em prever notas que ele nunca viu antes.

2. Modelo 1: Filtragem Colaborativa Baseada no Usu√°rio (User-Based)

‚Ä¢	O que faz: Constr√≥i e treina o primeiro modelo.

o	sim_options_user = ...: Define as op√ß√µes de similaridade.

ÔÇß	'name': 'cosine': Informa que usaremos a "Similaridade de Cossenos" para medir o qu√£o parecidos dois usu√°rios s√£o.

ÔÇß	'user_based': True: Este √© o comando principal. Ele diz ao modelo: "Encontre usu√°rios com gostos similares (que deram notas parecidas para os mesmos filmes) e recomende filmes que esses usu√°rios similares gostaram."

o	model_user = KNNBasic(...): Cria o modelo (KNNBasic) com essas op√ß√µes.

o	model_user.fit(trainset): Treina o modelo usando os dados de treino.

o	accuracy.rmse(predictions_user): Testa o modelo nos dados de teste e calcula as m√©tricas de erro RMSE (Raiz do Erro Quadr√°tico M√©dio) e MAE (Erro M√©dio Absoluto).

‚Ä¢	Por que: O objetivo √© testar a l√≥gica de "me diga o que pessoas parecidas comigo gostaram".

3. Modelo 2: Filtragem Colaborativa Baseada no Item (Item-Based)

‚Ä¢	O que faz: Constr√≥i e treina o segundo modelo, de forma muito parecida.

o	sim_options_item = ...: Define as op√ß√µes de similaridade.

ÔÇß	'user_based': False: Esta √© a mudan√ßa crucial. Agora, o modelo n√£o busca usu√°rios parecidos. Ele busca itens (filmes) parecidos.

o	A l√≥gica agora √©: "Se o usu√°rio gostou do filme X, encontre outros filmes (Y e Z) que s√£o 'similares' ao filme X (porque receberam notas parecidas dos mesmos usu√°rios) e recomende-os."

o	O restante (treino e teste) √© id√™ntico.

‚Ä¢	Por que: O objetivo √© testar a l√≥gica de "se voc√™ gostou disso, talvez goste daquilo".

A Sa√≠da (Resultados)

‚Ä¢	O que mostra: A sa√≠da imprime o RMSE e o MAE para ambos os modelos.

‚Ä¢	Por que: Essas m√©tricas medem o erro m√©dio das previs√µes. Quanto menor o n√∫mero, melhor o modelo.

o	No seu resultado, o modelo Item-Based (RMSE: 1.0876, MAE: 1.0250) teve um erro menor que o User-Based (RMSE: 1.7150, MAE: 1.3333).

o	Conclus√£o: Para este dataset espec√≠fico, a abordagem de "recomendar filmes similares" (Item-Based) foi mais precisa do que a de "recomendar o que usu√°rios similares gostam" (User-Based).


<img width="886" height="499" alt="image" src="https://github.com/user-attachments/assets/befcf550-3275-4c01-ab40-f946cea737d4" />


este bloco, o c√≥digo muda de abordagem. Ele sai da biblioteca surprise e come√ßa a preparar os dados para um modelo de Deep Learning (Rede Neural), que √© uma forma mais moderna de sistema de recomenda√ß√£o.

Esta etapa √© toda sobre Pr√©-processamento e Transforma√ß√£o de Dados.

1. Divis√£o dos Dados (Treino/Teste)

‚Ä¢	O que faz: O train_test_split √© usado para dividir o DataFrame original (df) em dois conjuntos: train_data (80% dos dados) e test_data (20% dos dados).

‚Ä¢	Por que: Esta √© uma regra fundamental. O modelo s√≥ pode "aprender" com os dados de treino. Os dados de teste s√£o guardados "em segredo" e s√≥ s√£o usados no final, para verificar se o modelo realmente aprendeu a generalizar ou se apenas "decorou" as respostas do treino.

2. Cria√ß√£o da Matriz Usu√°rio-Item

‚Ä¢	O que faz: O comando .pivot_table() transforma a lista de avalia√ß√µes em uma matriz (uma grande tabela).

o	As linhas se tornam os user_id.

o	As colunas se tornam os movie_id.

o	Os valores dentro da tabela s√£o os rating (notas).

‚Ä¢	Por que: Redes neurais e outros algoritmos de "Matrix Factorization" (Fatora√ß√£o Matricial) n√£o trabalham com listas, eles precisam dessa matriz "usu√°rio-item" como entrada. O .fillna(0) √© usado para preencher com 0 os filmes que um usu√°rio ainda n√£o avaliou.

3. Normaliza√ß√£o dos Dados

‚Ä¢	O que faz: O MinMaxScaler √© aplicado na matriz de treino. Ele pega todos os valores (que v√£o de 0 a 5) e os "comprime" para que fiquem na escala de 0 a 1.

‚Ä¢	Por que: Redes Neurais treinam de forma muito mais r√°pida, est√°vel e eficiente quando todos os n√∫meros de entrada est√£o em uma escala pequena e consistente, como 0 a 1.

A Sa√≠da

‚Ä¢	O que mostra: Dimens√µes da matriz de treino: (5, 4)

‚Ä¢	Por que: Confirma que, ap√≥s todo o processamento, a matriz de treino (que ser√° usada para alimentar a rede neural) tem 5 usu√°rios (linhas) e 4 filmes (colunas).


<img width="886" height="1103" alt="image" src="https://github.com/user-attachments/assets/21588552-8256-46af-9421-3daf1293ca20" />

 

Este bloco de c√≥digo √© onde o modelo de Deep Learning (Rede Neural) √© de fato constru√≠do. O modelo usado aqui √© um tipo especial chamado Autoencoder.

Um Autoencoder tem duas partes:

1.	Encoder (Codificador): Pega os dados de entrada (a linha de avalia√ß√µes do usu√°rio) e os comprime em uma representa√ß√£o muito pequena, chamada de "espa√ßo latente" ou "gargalo".

2.	Decoder (Decodificador): Pega essa representa√ß√£o comprimida ("gargalo") e tenta reconstruir a entrada original a partir dela.

An√°lise Detalhada do Bloco:

1. Arquitetura da Rede

‚Ä¢	n_movies = ...shape[1]: Primeiro, o c√≥digo verifica quantas colunas (filmes) a matriz de treino tem. A rede neural precisa saber disso.

‚Ä¢	input_layer = Input(...): Define a camada de entrada. Ela tem o mesmo n√∫mero de neur√¥nios que o n√∫mero de filmes (neste caso, 4, como visto no passo anterior).

‚Ä¢	Encoder (O Caminho de Compress√£o):

o	encoded = Dense(128, ...)

o	encoded = Dense(64, ...)

o	latent_view = Dense(32, ...)

o	O que faz: O input_layer com 4 neur√¥nios √© progressivamente "espremido" em camadas com 128, 64 e, finalmente, 32 neur√¥nios. Essa camada de 32 neur√¥nios √© o "gargalo" (latent view). Ela √© for√ßada a aprender uma representa√ß√£o super comprimida, mas muito rica, dos gostos daquele usu√°rio.

‚Ä¢	Decoder (O Caminho de Reconstru√ß√£o):

o	decoded = Dense(64, ...)

o	decoded = Dense(128, ...)

o	output_layer = Dense(n_movies, activation='sigmoid')

o	O que faz: O c√≥digo pega o "gargalo" de 32 neur√¥nios e faz o caminho inverso, "descomprimindo" de volta para 64, 128 e, finalmente, para o n√∫mero original de filmes (4).

o	Ponto-chave (activation='sigmoid'): A √∫ltima camada usa a ativa√ß√£o sigmoid. Isso √© feito de prop√≥sito, porque sigmoid sempre retorna um valor entre 0 e 1. Como normalizamos nossos dados para ficarem entre 0 e 1 no passo anterior, isso ajuda o modelo a gerar previs√µes na mesma escala.

2. Compila√ß√£o do Modelo

‚Ä¢	O que faz: O autoencoder.compile(...) prepara o modelo para o treino.

‚Ä¢	Por que:

o	optimizer=Adam(...): Define o otimizador Adam, que √© um algoritmo eficiente para ajustar os pesos da rede.

o	loss='mean_squared_error': Esta √© a parte mais importante. O "objetivo" do modelo (loss) √© o Erro Quadr√°tico M√©dio. O modelo tentar√° minimizar a diferen√ßa entre a input_layer (as notas reais) e a output_layer (as notas reconstru√≠das). Basicamente, ele ser√° treinado para ficar muito bom em "adivinhar" as notas que o usu√°rio deu.

A Sa√≠da (Model Summary)

‚Ä¢	O que mostra: O .summary() imprime um resumo da arquitetura, mostrando cada camada, seu formato de sa√≠da e o n√∫mero de par√¢metros (pesos) que ela precisa aprender.

‚Ä¢	Total params: 21,924: Informa que este modelo tem 21.924 "bot√µes" (par√¢metros) que ser√£o ajustados durante o treinamento para minimizar o erro.


 <img width="719" height="1459" alt="image" src="https://github.com/user-attachments/assets/9b0f1d92-4e1a-4cb3-86ad-d4520033c940" />

Este bloco de c√≥digo √© uma vers√£o completa e corrigida dos passos anteriores, juntando a prepara√ß√£o dos dados, a constru√ß√£o do modelo, o treinamento e a avalia√ß√£o (as partes que faltavam).

________________________________________

An√°lise Detalhada do Bloco:

1. Divis√£o e Prepara√ß√£o dos Dados (Revis√£o Correta)

Este trecho de c√≥digo refaz a prepara√ß√£o dos dados de forma mais robusta para evitar "vazamento de dados" (data leakage).

‚Ä¢	train_test_split (Feito Primeiro): O DataFrame original √© dividido em treino e teste antes de qualquer outra transforma√ß√£o. Isso garante que o modelo n√£o tenha nenhuma informa√ß√£o sobre o conjunto de teste durante o seu treinamento.

‚Ä¢	Cria√ß√£o das Matrizes: As matrizes usu√°rio-item de treino e teste s√£o criadas separadamente a partir dos dados j√° divididos.

‚Ä¢	Alinhamento (.reindex): Um passo crucial √© adicionado. O .reindex for√ßa a matriz de teste (user_item_matrix_test_raw) a ter exatamente as mesmas colunas (filmes) e linhas (usu√°rios) que a matriz de treino. Isso evita erros se, por acaso, um filme ou usu√°rio s√≥ existir no conjunto de teste.

‚Ä¢	Normaliza√ß√£o (MinMaxScaler Corrigida):

o	scaler.fit_transform(user_item_matrix_train): O MinMaxScaler √© treinado (fit) e aplicado (transform) apenas nos dados de treino. Ele "aprende" qual √© o valor m√≠nimo e m√°ximo (0 e 5) com os dados de treino.

o	scaler.transform(user_item_matrix_test): O scaler (j√° treinado) √© apenas aplicado (transform) nos dados de teste. Isso garante que os dados de teste sejam normalizados usando a mesma regra dos dados de treino, sem "contamin√°-los".

2. Defini√ß√£o do Modelo Autoencoder

Esta √© a arquitetura da rede neural, similar √† do passo anterior.

‚Ä¢	Arquitetura: O modelo √© um Autoencoder. Ele possui:

o	Encoder (Codificador): Comprime a entrada (n√∫mero de filmes) em camadas menores (Dense(64)) at√© um "gargalo" (latent_view = Dense(32)). Esta camada de 32 neur√¥nios √© uma representa√ß√£o compacta dos gostos do usu√°rio.

o	Decoder (Decodificador): Pega o "gargalo" (latent_view) e tenta reconstruir a entrada original, "descomprimindo-a" de volta (Dense(64) e Dense(n_movies)).

‚Ä¢	Compila√ß√£o: O modelo √© compilado com:

o	loss='mean_squared_error': A fun√ß√£o de perda que o modelo tentar√° minimizar (o erro entre a entrada e a sa√≠da reconstru√≠da).

o	metrics=['rmse']: Al√©m da perda, pedimos para ele monitorar o RMSE (RootMeanSquaredError), que √© uma m√©trica mais f√°cil de interpretar por estar na mesma unidade das notas.

3. Treinamento do Modelo

Esta √© a etapa onde o modelo "aprende".

‚Ä¢	autoencoder.fit(...): Inicia o treinamento.

o	x=X_train_scaled: Os dados de entrada.

o	y=X_train_scaled: Os dados de "sa√≠da esperada". No Autoencoder, a entrada e a sa√≠da s√£o as mesmas. O objetivo do modelo √© aprender a recriar o seu pr√≥prio input da forma mais fiel poss√≠vel.

o	epochs=100: O modelo ver√° o dataset de treino 100 vezes.

o	batch_size=32: O modelo treina em "lotes" de 32 usu√°rios por vez.

o	validation_data=(X_test_scaled, X_test_scaled): Ao final de cada √©poca, o modelo tentar√° prever as notas do conjunto de teste (que ele nunca usou para treinar) e reportar√° o erro. Isso √© vital para monitorar se o modelo est√° generalizando bem ou apenas "decorando" os dados de treino (overfitting).

4. Avalia√ß√£o do Desempenho

Ap√≥s o fim do treinamento, este bloco calcula o desempenho final e "real" do modelo.

‚Ä¢	autoencoder.evaluate(X_test_scaled, X_test_scaled): Roda o modelo uma √∫ltima vez sobre o conjunto de teste.

‚Ä¢	Resultados: A sa√≠da imprime a "Perda (MSE)" e o "RMSE" finais. O RMSE √© a m√©trica principal: ele nos diz, em m√©dia, o qu√£o longe as previs√µes de notas do modelo (a sa√≠da reconstru√≠da) ficaram das notas reais do conjunto de teste. Quanto menor o RMSE, melhor o modelo.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Quest√£o 9 (Raio-X com CNNs)

Configura√ß√£o e Desbalanceamento 

<img width="886" height="552" alt="image" src="https://github.com/user-attachments/assets/3182aee3-c411-421a-a4d5-45f3e44559f1" />

 
‚Ä¢	(Configura√ß√£o): O primeiro passo foi a organiza√ß√£o. Importamos as bibliotecas (TensorFlow, Keras, Pandas) e definimos os caminhos para os dados de treino, teste e valida√ß√£o no Kaggle.

<img width="643" height="630" alt="image" src="https://github.com/user-attachments/assets/36bcefae-523a-4646-826d-e5c4ada80504" />


‚Ä¢	(Corre√ß√£o do Desbalanceamento): Logo de cara, identificamos um problema cr√≠tico nos dados: t√≠nhamos muito mais imagens de 'Pneumonia' (3875) do que 'Normais' (1341).

o	Por qu√™ isso √© um problema? Se ignor√°ssemos isso, o modelo ficaria 'viciado' em chutar 'Pneumonia' e teria uma p√©ssima performance em identificar pacientes saud√°veis.o	Como resolvemos? Calculamos class_weights. Isso d√° um 'peso' maior (1.94x) para cada amostra 'Normal', for√ßando o modelo a prestar mais aten√ß√£o nela durante o treino."

‚Ä¢ Constru√ß√£o e Compila√ß√£o da CNN 

 <img width="642" height="545" alt="image" src="https://github.com/user-attachments/assets/611c2a61-405a-49e8-8907-fbe4792c2115" />
 
‚Ä¢ (Construindo o Modelo): Usamos uma Rede Neural Convolucional (CNN).

o	Como? Empilhamos camadas Conv2D (para achar padr√µes) e MaxPooling2D (para reduzir a imagem e focar no que √© importante).

o	Por qu√™? Esse processo permite que a rede aprenda caracter√≠sticas, desde bordas simples at√© padr√µes complexos. Tamb√©m usamos BatchNormalization para estabilizar o treino e Dropout para evitar que o modelo decore as imagens (overfitting).

<img width="490" height="591" alt="image" src="https://github.com/user-attachments/assets/3a8f54a6-430f-4e07-bcfc-9387065c2f2d" />
 

‚Ä¢ (Compilando o Modelo): Com a arquitetura pronta, 'compilamos' o modelo.

o	Como? Definimos o otimizador Adam (com uma taxa de aprendizado baixa para um ajuste fino), a fun√ß√£o de perda binary_crossentropy (ideal para classifica√ß√£o sim/n√£o) e a m√©trica accuracy."

‚Ä¢ Geradores de Dados e Treinamento

<img width="638" height="605" alt="image" src="https://github.com/user-attachments/assets/ef7d1a72-e30b-4edd-bd72-b0cb4c6f3936" />

 
‚Ä¢	(Preparando Geradores): Usamos o ImageDataGenerator.

o	Por qu√™? Para aplicar Data Augmentation. O modelo poderia memorizar as poucas imagens de treino.

o	Como? O gerador de treino aplica zoom, rota√ß√µes e giros aleat√≥rios em tempo real. Isso 'cria' novas imagens e ensina o modelo a generalizar. O gerador de teste n√£o faz isso, apenas redimensiona as imagens.

<img width="856" height="253" alt="image" src="https://github.com/user-attachments/assets/e0c46c2d-2d54-46a6-9a0b-17d8ad197ac0" />


‚Ä¢	(Treinando o Modelo): Aqui, usamos o model.fit().

o	Como? Passamos os geradores, as 25 √©pocas e, crucialmente, os class_weights que calculamos l√° no Bloco 2. Agora o treino √© justo."

‚Ä¢ Avalia√ß√£o do Modelo 

<img width="886" height="702" alt="image" src="https://github.com/user-attachments/assets/1b329f89-7542-4a7e-a773-125fd18c76fb" />

 
‚Ä¢	Como? Usamos o classification_report e a confusion_matrix nos dados de teste (que o modelo nunca viu).

‚Ä¢	Por qu√™? O report nos deu m√©tricas vitais como precis√£o (quantos 'Pneumonia' que previmos estavam certos) e recall (quantos 'Pneumonia' reais n√≥s conseguimos encontrar). A Matriz de Confus√£o mostrou visualmente onde o modelo acertou e errou."

--------------------------------------------------------------------------------------------------------------------------------------

Quest√£o 10 (Previs√£o de Vendas)

‚Ä¢ Carga e Explora√ß√£o 

<img width="886" height="744" alt="image" src="https://github.com/user-attachments/assets/30b49606-b033-4919-9cd1-66667a5b80b5" />

 
‚Ä¢	(Carga): Come√ßamos carregando o store.csv com o Pandas. O df.head() e df.describe() nos mostraram que t√≠nhamos uma mistura de dados num√©ricos (Gastos) e categ√≥ricos (Tipo_Loja)."









‚Ä¢ Feature Engineering 

<img width="886" height="360" alt="image" src="https://github.com/user-attachments/assets/3129a588-354b-442f-a93e-cb0407a89918" />

 
‚Ä¢	(Normaliza√ß√£o): Vimos que 'Gastos_Publicidade' (milhares) e 'Numero_Funcionarios' (dezenas) tinham escalas muito diferentes.

o	Por qu√™? Modelos de regress√£o d√£o mais import√¢ncia a n√∫meros maiores, o que distorce a an√°lise.

o	Como? Usamos MinMaxScaler para colocar todas as colunas num√©ricas na mesma escala (entre 0 e 1).

<img width="886" height="422" alt="image" src="https://github.com/user-attachments/assets/a44c7f1f-cbda-4d64-ab88-9a94b01fcd98" />

 
‚Ä¢	(Encoding): O modelo n√£o entende o que 'Tipo_Loja_A' significa.

o	Como? Usamos One-Hot Encoding (get_dummies) para transformar colunas de texto em colunas num√©ricas (0 ou 1)."



‚Ä¢ Regress√£o Linear e √Årvore

  <img width="886" height="698" alt="image" src="https://github.com/user-attachments/assets/76955241-4550-4047-9a31-7e6198bff92e" />

  <img width="886" height="528" alt="image" src="https://github.com/user-attachments/assets/354fbc7c-185a-4efb-81db-7abd0b56ac07" />


‚Ä¢	(Split e Regress√£o Linear): Primeiro, separamos os dados em Treino e Teste (70/30). Em seguida, treinamos nosso primeiro modelo, a LinearRegression.

<img width="886" height="526" alt="image" src="https://github.com/user-attachments/assets/698322c6-b90a-40e6-ad03-4b38b874a65d" />

 
‚Ä¢	(Avalia√ß√£o): Avaliamos a Regress√£o Linear com MAE, RMSE e R¬≤. 

<img width="886" height="524" alt="image" src="https://github.com/user-attachments/assets/851036ae-b0e7-43ea-9593-f4f0cc0b9176" />

‚Ä¢ treinamos o segundo modelo, o DecisionTreeRegressor."

<img width="886" height="552" alt="image" src="https://github.com/user-attachments/assets/28146673-dea7-4115-897a-c8fa22ed52c3" />


‚Ä¢ Avalia√ß√£o e XGBoost 

<img width="886" height="511" alt="image" src="https://github.com/user-attachments/assets/f4886f01-9a3e-4f9d-b4ab-40ffec4a71e3" />

 
‚Ä¢	(Avalia√ß√£o): Avaliamos a √Årvore de Decis√£o. 

<img width="886" height="511" alt="image" src="https://github.com/user-attachments/assets/33fe28d6-f73f-4d9a-a891-f5748d7fe45e" />

 
‚Ä¢	(Avalia√ß√£o ): Por fim, treinamos e avaliamos o XGBRegressor, um modelo mais robusto. 

----------

‚Ä¢ Aqui n√≥s avaliamos as features de mais import√¢ncia nos modelos

<img width="886" height="644" alt="image" src="https://github.com/user-attachments/assets/7a40b9c6-734c-4d81-ab47-8b18dceaa2a0" />

<img width="886" height="780" alt="image" src="https://github.com/user-attachments/assets/a0628063-fc7e-4ab7-999c-2359c34ec7e1" />

<img width="886" height="796" alt="image" src="https://github.com/user-attachments/assets/96771a22-db0c-4242-a3f5-84b414c4c458" />




 
 
 


